{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7131c696-1918-4b43-8f53-034a4b56c4f7",
   "metadata": {},
   "source": [
    "# Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5e3cb-c4e7-402a-927e-a34bf56c6417",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384af6af-cee0-4ba7-a441-5af8472ed768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import csv\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from model.keypoint_classifier.keypoint_classifier import KeyPointClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee85e51-20b0-4d40-a4c0-5bfd41436dc8",
   "metadata": {},
   "source": [
    "### Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4faecb2-8fa5-4743-bce2-547e0b2f7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(use_static_image_mode=False, min_detection_confidence=0.7, min_tracking_confidence=0.5):\n",
    "        # Model load #############################################################\n",
    "        mp_hands = mp.solutions.hands\n",
    "        hands = mp_hands.Hands(\n",
    "            static_image_mode=use_static_image_mode,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=min_detection_confidence,\n",
    "            min_tracking_confidence=min_tracking_confidence,\n",
    "        )\n",
    "\n",
    "        keypoint_classifier = KeyPointClassifier()\n",
    "\n",
    "        # Read labels ###########################################################\n",
    "        with open('model/keypoint_classifier/keypoint_classifier_label_new.csv',\n",
    "                  encoding='utf-8-sig') as f:\n",
    "            keypoint_classifier_labels = csv.reader(f)\n",
    "            keypoint_classifier_labels = [\n",
    "                row[0] for row in keypoint_classifier_labels\n",
    "            ]\n",
    "\n",
    "        return hands, keypoint_classifier, keypoint_classifier_labels\n",
    "\n",
    "def _calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "    # print(image_width)\n",
    "    # print(image_height)\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        # landmark_z = landmark.z\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "    \n",
    "    return landmark_point\n",
    "\n",
    "def _pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "    \n",
    "    return temp_landmark_list\n",
    "    \n",
    "def recognize(image, hands, keypoint_classifier, keypoint_classifier_labels, number=-1, mode=0):\n",
    "    # TODO: Move constants to other place\n",
    "    USE_BRECT = True\n",
    "\n",
    "    image = cv2.flip(image, 1)  # Mirror display\n",
    "    debug_image = copy.deepcopy(image)\n",
    "\n",
    "    # Saving gesture id for drone controlling\n",
    "    gesture_id = -1\n",
    "\n",
    "    # Detection implementation #############################################################\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    #  ####################################################################\n",
    "    if results.multi_hand_landmarks is not None:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks,\n",
    "                                              results.multi_handedness):\n",
    "            # Bounding box calculation\n",
    "            # brect = self._calc_bounding_rect(debug_image, hand_landmarks)\n",
    "            # Landmark calculation\n",
    "            landmark_list = _calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "            # Conversion to relative coordinates / normalized coordinates\n",
    "            pre_processed_landmark_list = _pre_process_landmark(landmark_list)\n",
    "            # pre_processed_point_history_list = self._pre_process_point_history(\n",
    "            #     debug_image, self.point_history)\n",
    "\n",
    "            # Write to the dataset file\n",
    "            # self._logging_csv(number, mode, pre_processed_landmark_list, pre_processed_point_history_list)\n",
    "\n",
    "            # Hand sign classification\n",
    "            hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "            # print(\"Hand Sign: {}\".format(keypoint_classifier_labels[hand_sign_id]))\n",
    "            # if hand_sign_id == 2:  # Point gesture\n",
    "            #     self.point_history.append(landmark_list[8])\n",
    "            # else:\n",
    "            #     self.point_history.append([0, 0])\n",
    "\n",
    "            # Finger gesture classification\n",
    "#             finger_gesture_id = 0\n",
    "#             point_history_len = len(pre_processed_point_history_list)\n",
    "#             if point_history_len == (self.history_length * 2):\n",
    "#                 finger_gesture_id = self.point_history_classifier(\n",
    "#                     pre_processed_point_history_list)\n",
    "\n",
    "#             # Calculates the gesture IDs in the latest detection\n",
    "#             self.finger_gesture_history.append(finger_gesture_id)\n",
    "#             most_common_fg_id = Counter(\n",
    "#                 self.finger_gesture_history).most_common()\n",
    "\n",
    "#             # Drawing part\n",
    "#             debug_image = self._draw_bounding_rect(USE_BRECT, debug_image, brect)\n",
    "#             debug_image = self._draw_landmarks(debug_image, landmark_list)\n",
    "#             debug_image = self._draw_info_text(\n",
    "#                 debug_image,\n",
    "#                 brect,\n",
    "#                 handedness,\n",
    "#                 self.keypoint_classifier_labels[hand_sign_id],\n",
    "#                 self.point_history_classifier_labels[most_common_fg_id[0][0]]\n",
    "#             )\n",
    "\n",
    "            # Saving gesture\n",
    "            gesture_id = hand_sign_id\n",
    "    # else:\n",
    "    #     self.point_history.append([0, 0])\n",
    "\n",
    "    # debug_image = self.draw_point_history(debug_image, self.point_history)\n",
    "    \n",
    "    if gesture_id == -1:\n",
    "        hand_pose = \"NO HAND\"\n",
    "    else:\n",
    "        hand_pose = keypoint_classifier_labels[gesture_id]\n",
    "\n",
    "    return debug_image, gesture_id, hand_pose, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5f233d-acaf-4480-821d-73f146e58aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "hands, keypoint_classifier, keypoint_classifier_labels = load_model()\n",
    "\n",
    "# Hand model\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Drawing util\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "while cap.isOpened():\n",
    "    # ret = return value (not needed)\n",
    "    # frame = frame of footage\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    image = frame\n",
    "    \n",
    "    # Retrieve prediction for hand\n",
    "    debug, gesture_id, hand_sign, results = recognize(image = frame, hands = hands, keypoint_classifier = keypoint_classifier, \n",
    "                                  keypoint_classifier_labels = keypoint_classifier_labels)\n",
    "    \n",
    "    # Read an image, flip it around y-axis for correct handedness output\n",
    "    image = cv2.flip(image, 1)\n",
    "\n",
    "    # Set flag to true\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    # Detections\n",
    "    # print(results)\n",
    "\n",
    "    # Rendering results\n",
    "    if results.multi_hand_landmarks:\n",
    "        for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "            # mp_hands.HAND_CONNECTIONS -> telling Mediapipe the relationship between the landmarks of the hand\n",
    "            # mp_drawing.DrawingSpec -> telling Mediapipe how to draw the hands onto a given frame. Note: colour \n",
    "            # here is in BGR format\n",
    "            mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS, \n",
    "                                    mp_drawing.DrawingSpec(color=(157, 212, 76), thickness=2, circle_radius=2),\n",
    "                                    mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                                     )\n",
    "\n",
    "    # cv2.putText(image, hand_sign, (200,450), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Display frame\n",
    "    cv2.imshow('Hand Tracking', image)\n",
    "\n",
    "    # Breaks loop if we enter 'q'\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb882a4-d416-4818-a4d2-14c7d114eb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
